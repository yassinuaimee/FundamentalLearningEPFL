{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "**What will you learn today**: Backpropagation is the fundamental building block of deep learning, as it allows to automatically compute the gradients of any arbitrary compositional function with a similar computational cost than evaluating it. This makes possible to work with arbitrarily complex neural network architectures, composed of many layers, without the need to manually compute their gradients. Backpropagation is already implemented in all high-level deep learning frameworks, e.g. `PyTorch`, and as such, we would hardly ever need to think of how it works. However, it is a very educational exercise to implement it once in your life, and that is precisely what we will do in this exercise! In particular, you will learn to implement and derive the forward and backward pass of a very simple neural network in pure `numpy`. As a bonus, we will also explore how to approximate the non-convex loss landscape of a neural network by a convex one, and we will learn how to use such approximation to derive intuitions about how different design choices affect the network's behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Forward pass\n",
    "To simplify the exercise, we will only work with a simple architecture, consisting of a feedforward neural network with two fully connected layers, i.e., a single-hidden layer MLP.\n",
    "\n",
    "![simple_mlp](./simple_mlp.png)\n",
    "\n",
    "Mathematically, we can write the feedforward computation as:\n",
    "$$ x_j^{(1)}=\\sigma\\left(z_j^{(1)}\\right)=\\sigma\\left(\\sum_{i=1}^D w_{i,j}^{(1)} x_i^{(0)}+b_j^{(1)}\\right), $$\n",
    "$$ \\hat y =\\sigma\\left(z_1^{(2)}\\right)=\\sigma\\left(\\sum_{i=1}^K w_{i,1}^{(2)} x_i^{(1)}+b_1^{(2)}\\right),  $$\n",
    "where $\\sigma(\\cdot)$ denotes the sigmoid activation function. In the rest of the exercise, we will use $D=4$, and $K=5$.\n",
    "\n",
    "We can alternatively write the same computation in vector notation\n",
    "$$ \\bf x^{(1)}=\\sigma\\left(\\bf z^{(1)}\\right)=\\sigma\\left(\\bf W^{(1)} \\bf x^{(0)}+\\bf b^{(1)}\\right), $$\n",
    "$$ \\hat y=\\sigma\\left(z^{(2)}\\right)=\\sigma\\left(\\bf {w^{(2)}}^\\top \\bf x^{(1)}+b^{(2)}\\right). $$\n",
    "\n",
    "In general, we will denote the function computed by the neural network as $f_{\\bf w}(\\bf x)=\\hat y$, and use $\\bf w$ to represent the vector of all weights in the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "As a warm up, let's implement the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def grad_sigmoid(t):\n",
    "    \"\"\"return the derivative of sigmoid on t.\"\"\"\n",
    "    return sigmoid(t) * (1 - sigmoid(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize our data and parameters with some random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.01, 0.02, 0.03, 0.04])\n",
    "W = {\n",
    "    \"w_1\": np.ones((4, 5)),\n",
    "    \"w_2\": np.ones(5)\n",
    "}\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's implement the forward pass. If you implement it correctly, you should see that your code can pass the test successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your implementation is not correct.\n"
     ]
    }
   ],
   "source": [
    "def simple_feed_forward(x, W):\n",
    "    \"\"\"Do feed forward propagation.\"\"\" \n",
    "    x_0 = x\n",
    "    z_1 = W[\"w_1\"].T @ x_0\n",
    "    x_1 = sigmoid(z_1)\n",
    "    z_2 = W[\"w_2\"].T @ x_1\n",
    "    y_hat = sigmoid(z_2)\n",
    "    \n",
    "    return z_1, z_2, y_hat\n",
    "\n",
    "try:\n",
    "    expected = 0.93244675427215695\n",
    "    _, _, yours = simple_feed_forward(x, W)\n",
    "    assert np.sum((yours - expected) ** 2) < 1e-15\n",
    "    print(\"Your implementation is correct!\")\n",
    "except:\n",
    "    print(\"Your implementation is not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Backward pass\n",
    "\n",
    "We now have a working implementation of our network! However, if we want to be able to train it using gradient descent, we need to be able to compute its gradient. Let's do that.\n",
    "\n",
    "We will use the squared error as our loss function, i.e.,\n",
    "$$\\ell(y,\\hat y)=\\frac{1}{2}(\\hat y-y)^2$$\n",
    "\n",
    "\n",
    "## Exercise\n",
    "Evaluate the derivative of $\\mathcal{L}(\\bf w)=\\ell(y, f_{\\bf w}(\\bf x))$ with respect to $w_{i,1}^{(2)}$ and $w_{i,j}^{(1)}$ for a single training sample $(\\bf x, y)$, by following the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the derivations seen in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now that we have derived the backward pass analytically, let's implement it in Python!\n",
    "\n",
    "*Hint*: You might want to slightly change `simple_feed_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_backpropagation(y, x, W):\n",
    "    \"\"\"Do backpropagation and get delta_W.\"\"\"\n",
    "    # Feed forward\n",
    "    z_1, z_2, y_hat = simple_feed_forward(x, W)\n",
    "    x_1 = sigmoid(z_1)\n",
    "    # Backpropogation\n",
    "    delta_2 = (y_hat - y) * grad_sigmoid(z_2)\n",
    "    delta_w_2 = delta_2 * x_1\n",
    "    delta_1 = delta_2 * W[\"w_2\"] * grad_sigmoid(z_1)\n",
    "    delta_w_1 = np.outer(x, delta_1)\n",
    "    return {\n",
    "        \"w_2\": delta_w_2,\n",
    "        \"w_1\": delta_w_1\n",
    "    }\n",
    "  \n",
    "try:\n",
    "    expected = {\n",
    "        'w_1': np.array([\n",
    "            [ -1.06113639e-05,  -1.06113639e-05,  -1.06113639e-05, -1.06113639e-05,  -1.06113639e-05],\n",
    "            [ -2.12227277e-05,  -2.12227277e-05,  -2.12227277e-05, -2.12227277e-05,  -2.12227277e-05],\n",
    "            [ -3.18340916e-05,  -3.18340916e-05,  -3.18340916e-05, -3.18340916e-05,  -3.18340916e-05],\n",
    "            [ -4.24454555e-05,  -4.24454555e-05,  -4.24454555e-05, -4.24454555e-05,  -4.24454555e-05]]),\n",
    "        'w_2': np.array(\n",
    "            [-0.00223387, -0.00223387, -0.00223387, -0.00223387, -0.00223387])\n",
    "    }\n",
    "    yours = simple_backpropagation(y, x, W)\n",
    "    assert np.sum(\n",
    "        [np.sum((yours[key] - expected[key]) ** 2) for key in expected.keys()]) < 1e-15\n",
    "    print(\"Your implementation is correct!\")\n",
    "except:\n",
    "    print(\"Your implementation is not correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Bonus: Effect of regularization\n",
    "\n",
    "One of the first things we learn about neural networks is that their loss landscape is not convex. This means that analyzing how different design choices will affect their performance precisely is generally very hard. Fortunately, however, many times we can get an intuition of the behaviour of a neural network by taking a few approximations. We will now explore one of those. In particular, we will use some simple approximations to explore what is the effect of regularization on the weights of a neural network. \n",
    "\n",
    "Let $\\bf w$ be the weight vetor of all weights in the neural network, and recall that we do not normally penalize the bias term, so let's ignore it for the rest of our derivations. Furthermore, let $\\bf w^\\star$ denote a parameter that minimizes the cost function $\\mathcal L$ for the given test set (where the cost functions does not include the regularization). We would like to study how the optimal weight changes if we include some regularization.\n",
    "\n",
    "In order to make the problem tractable, assume that $\\mathcal L(\\bf w)$ can be locally expanded around the optimal parrameter $\\bf w^\\star$ in the form\n",
    "$$\\mathcal L(\\bf w) =\\mathcal L(\\bf w^\\star)+\\frac{1}{2}(\\bf w-\\bf w^\\star)^\\top\\bf H(\\bf w-\\bf w^\\star),$$\n",
    "where $\\bf H$ denotes the Hessian, whose components are the entries\n",
    "$$\\cfrac{\\partial^2 \\mathcal{L}}{\\partial \\bf w_i \\partial \\bf w_j }$$\n",
    "\n",
    "Now, let's add a regularization term of the form $\\frac{1}{2}\\mu\\|\\bf w\\|^2_2$.\n",
    "\n",
    "## Exercise\n",
    "1. Show that the optimum weight vector for the regularized problem is given by $$\\bf Q(\\bf \\Lambda+\\mu\\bf I)^{-1}\\bf \\Lambda\\bf Q^\\top \\bf w^\\star$$ where $\\bf H=\\bf Q\\bf\\Lambda\\bf Q^\\top$ represents the eigenvalue decomposition of the symmetric matrix $\\bf H$, i.e., $\\bf Q$ is an orthonormal matrix, and $\\bf \\Lambda$ is a diagonal matrix whose entries are non-negative and decreasing along the diagonal.\n",
    "2. Show that $(\\bf\\Lambda+\\mu\\bf I)^{-1}\\bf\\Lambda$ is again a diagonal matrix whose $i$-th entry is now $\\lambda_i/(\\lambda_i+\\mu)$.\n",
    "3. Argue that along the dimensions of the eigenvectors of $\\bf H$ that correspond to large eigenvalues, essentially no changes occur in the weights, but that along the dimensions of eigenvectors of very small eigenvalues the weight is drastically decreased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "1. We are minimizing $\\mathcal L(\\bf w)+\\frac{\\mu}{2}\\|\\bf w\\|^2_2$. If we use the specific form of $\\mathcal L$ ad take the derivative, we get the first order condition of optimality $$\\bf H(\\bf w-\\bf w^\\star)=\\bf 0.$$\n",
    "Solving for $\\bf w$, gives us $\\hat{\\bf w}=(\\bf H +\\mu\\bf I)^{-1}\\bf H\\bf w^\\star$. If we now insert the eigendecomposition of $\\bf H$ this gives us the stated solution.\n",
    "2. Note that $\\bf \\Lambda+\\mu \\bf I$ is a diagonal matrix with entries along the diagonal equal to $\\lambda_i+\\mu$. The inverse of a diagonal matrix is again a diagonal matrix whose entries are the inverses of the entries of the matrix to be inverted. Hence, for our case $(\\Lambda+\\mu\\bf I)^{-1}$ is a diagonal matrix with entries $1/(\\lambda_i+\\mu)$. Finally the multiplication of two diagonal matrices is again a diagonal matrix whose entries are just the product of the diagonal entries.\n",
    "3. Now note that if all $\\lambda_i$ were very large, then all entries of the diagonal matrix $(\\bf \\Lambda +\\mu \\bf I)^{-1}\\bf\\Lambda$ would be very close to $1$. Hence, $\\bf Q(\\bf \\Lambda +\\mu \\bf I)^{-1}\\bf\\Lambda\\bf Q^\\top$ would be essentially an identity matrix (check what happens if we apply that matrix to the vector $\\bf w^\\star$).\n",
    "However, if some eigenvalues $\\lambda_i$ are close to $0$ then $\\lambda_i/(\\lambda_i+\\mu)$ is close to zero as well. Hence, after rotation $\\bf w^\\star$ by multiplying it with the unitary matrix $\\bf Q^\\top$, some of the componentes in the new basis are essentially set to zero. Thus, we are effectively projecting the components of $\\bf w^\\star$ away from the subspace of smallest curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('playground')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b6258ac5f9ce9aee8bc7e8f09e746bac739d42425ff8343fe5df569d2e6cb19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
