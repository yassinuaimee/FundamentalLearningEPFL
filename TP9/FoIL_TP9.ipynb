{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "**What will you learn today**: Backpropagation is the fundamental building block of deep learning, as it allows to automatically compute the gradients of any arbitrary compositional function with a similar computational cost than evaluating it. This makes possible to work with arbitrarily complex neural network architectures, composed of many layers, without the need to manually compute their gradients. Backpropagation is already implemented in all high-level deep learning frameworks, e.g. `PyTorch`, and as such, we would hardly ever need to think of how it works. However, it is a very educational exercise to implement it once in your life, and that is precisely what we will do in this exercise! In particular, you will learn to implement and derive the forward and backward pass of a very simple neural network in pure `numpy`. As a bonus, we will also explore how to approximate the non-convex loss landscape of a neural network by a convex one, and we will learn how to use such approximation to derive intuitions about how different design choices affect the network's behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Forward pass\n",
    "To simplify the exercise, we will only work with a simple architecture, consisting of a feedforward neural network with two fully connected layers, i.e., a single-hidden layer MLP.\n",
    "\n",
    "![simple_mlp](./simple_mlp.png)\n",
    "\n",
    "Mathematically, we can write the feedforward computation as:\n",
    "$$ x_j^{(1)}=\\sigma\\left(z_j^{(1)}\\right)=\\sigma\\left(\\sum_{i=1}^D w_{i,j}^{(1)} x_i^{(0)}+b_j^{(1)}\\right), $$\n",
    "$$ \\hat y =\\sigma\\left(z_1^{(2)}\\right)=\\sigma\\left(\\sum_{i=1}^K w_{i,1}^{(2)} x_i^{(1)}+b_1^{(2)}\\right),  $$\n",
    "where $\\sigma(\\cdot)$ denotes the sigmoid activation function. In the rest of the exercise, we will use $D=4$, and $K=5$.\n",
    "\n",
    "We can alternatively write the same computation in vector notation\n",
    "$$ \\bf x^{(1)}=\\sigma\\left(\\bf z^{(1)}\\right)=\\sigma\\left(\\bf W^{(1)} \\bf x^{(0)}+\\bf b^{(1)}\\right), $$\n",
    "$$ \\hat y=\\sigma\\left(z^{(2)}\\right)=\\sigma\\left(\\bf {w^{(2)}}^\\top \\bf x^{(1)}+b^{(2)}\\right). $$\n",
    "\n",
    "In general, we will denote the function computed by the neural network as $f_{\\bf w}(\\bf x)=\\hat y$, and use $\\bf w$ to represent the vector of all weights in the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "As a warm up, let's implement the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def grad_sigmoid(t):\n",
    "    \"\"\"return the derivative of sigmoid on t.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize our data and parameters with some random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.01, 0.02, 0.03, 0.04])\n",
    "W = {\n",
    "    \"w_1\": np.ones((4, 5)),\n",
    "    \"w_2\": np.ones(5)\n",
    "}\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's implement the forward pass. If you implement it correctly, you should see that your code can pass the test successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_feed_forward(x, W):\n",
    "    \"\"\"Do feed-forward propagation.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # You should at least return y_hat: a scalar.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    return y_hat\n",
    "\n",
    "try:\n",
    "    expected = 0.93244675427215695\n",
    "    yours = simple_feed_forward(x, W)\n",
    "    assert np.sum((yours - expected) ** 2) < 1e-15\n",
    "    print(\"Your implementation is correct!\")\n",
    "except:\n",
    "    print(\"Your implementation is not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Backward pass\n",
    "\n",
    "We now have a working implementation of our network! However, if we want to be able to train it using gradient descent, we need to be able to compute its gradient. Let's do that.\n",
    "\n",
    "We will use the squared error as our loss function, i.e.,\n",
    "$$\\ell(y,\\hat y)=\\frac{1}{2}(\\hat y-y)^2$$\n",
    "\n",
    "\n",
    "## Exercise\n",
    "Evaluate the derivative of $\\mathcal{L}(\\bf w)=\\ell(y, f_{\\bf w}(\\bf x))$ with respect to $w_{i,1}^{(2)}$ and $w_{i,j}^{(1)}$ for a single training sample $(\\bf x, y)$, by following the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write down your solution in this cell...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now that we have derived the backward pass analytically, let's implement it in Python!\n",
    "\n",
    "*Hint*: You might want to slightly change `simple_feed_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_backpropagation(y, x, W):\n",
    "    \"\"\"Do backpropagation and get delta_W.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError    \n",
    "    return {\n",
    "        \"w_1\": delta_w_1,\n",
    "        \"w_2\": delta_w_2\n",
    "    }\n",
    "    \n",
    "try:\n",
    "    expected = {\n",
    "        'w_1': np.array([\n",
    "            [ -1.06113639e-05,  -1.06113639e-05,  -1.06113639e-05, -1.06113639e-05,  -1.06113639e-05],\n",
    "            [ -2.12227277e-05,  -2.12227277e-05,  -2.12227277e-05, -2.12227277e-05,  -2.12227277e-05],\n",
    "            [ -3.18340916e-05,  -3.18340916e-05,  -3.18340916e-05, -3.18340916e-05,  -3.18340916e-05],\n",
    "            [ -4.24454555e-05,  -4.24454555e-05,  -4.24454555e-05, -4.24454555e-05,  -4.24454555e-05]]),\n",
    "        'w_2': np.array(\n",
    "            [-0.00223387, -0.00223387, -0.00223387, -0.00223387, -0.00223387])\n",
    "    }\n",
    "    yours = simple_backpropagation(y, x, W)    \n",
    "    assert np.sum(\n",
    "        [np.sum((yours[key] - expected[key]) ** 2)\n",
    "         for key in expected.keys()]) < 1e-15\n",
    "    print(\"Your implementation is correct!\")\n",
    "except:\n",
    "    print(\"Your implementation is not correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Bonus: Effect of regularization\n",
    "\n",
    "One of the first things we learn about neural networks is that their loss landscape is not convex. This means that analyzing how different design choices will affect their performance precisely is generally very hard. Fortunately, however, many times we can get an intuition of the behaviour of a neural network by taking a few approximations. We will now explore one of those. In particular, we will use some simple approximations to explore what is the effect of regularization on the weights of a neural network. \n",
    "\n",
    "Let $\\bf w$ be the weight vetor of all weights in the neural network, and recall that we do not normally penalize the bias term, so let's ignore it for the rest of our derivations. Furthermore, let $\\bf w^\\star$ denote a parameter that minimizes the cost function $\\mathcal L$ for the given test set (where the cost functions does not include the regularization). We would like to study how the optimal weight changes if we include some regularization.\n",
    "\n",
    "In order to make the problem tractable, assume that $\\mathcal L(\\bf w)$ can be locally expanded around the optimal parrameter $\\bf w^\\star$ in the form\n",
    "$$\\mathcal L(\\bf w) =\\mathcal L(\\bf w^\\star)+\\frac{1}{2}(\\bf w-\\bf w^\\star)^\\top\\bf H(\\bf w-\\bf w^\\star),$$\n",
    "where $\\bf H$ denotes the Hessian, whose components are the entries\n",
    "$$\\cfrac{\\partial^2 \\mathcal{L}}{\\partial \\bf w_i \\partial \\bf w_j }$$\n",
    "\n",
    "Now, let's add a regularization term of the form $\\frac{1}{2}\\|\\bf w\\|^2_2$.\n",
    "\n",
    "## Exercise\n",
    "1. Show that the optimum weight vector for the regularized problem is given by $$\\bf Q(\\bf \\Lambda+\\mu\\bf I)^{-1}\\bf\\Lambda\\bf Q^\\top \\bf w^\\star$$ where $\\bf H=\\bf Q\\bf\\Lambda\\bf Q^\\top$ represents the eigenvalue decomposition of the symmetric matrix $\\bf H$, i.e., $\\bf Q$ is an orthonormal matrix, and $\\bf \\Lambda$ is a diagonal matrix whose entries are non-negative and decreasing along the diagonal.\n",
    "2. Show that $(\\bf\\Lambda+\\mu\\bf I)^{-1}\\bf\\Lambda$ is again a diagonal matrix whose $i$-th entry is now $\\lambda_i/(\\lambda_i+\\mu)$.\n",
    "3. Argue that along the dimensions of the eigenvectors of $\\bf H$ that correspond to large eigenvalues, essentially no changes occur in the weights, but that along the dimensions of eigenvectors of very small eigenvalues the weight is drastically decreased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write down your solution in this cell...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
